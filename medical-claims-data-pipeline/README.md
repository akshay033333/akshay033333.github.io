# Medical Claims Data Pipeline

## ğŸ¥ Project Overview

A comprehensive end-to-end data engineering solution for processing and analyzing medical claims data using Apache Spark, Apache Kafka, and modern cloud infrastructure. This project demonstrates real-time streaming data processing, batch analytics, and infrastructure as code practices for healthcare data analytics.

## ğŸ“‹ Abstract

The Medical Claims Data Pipeline is designed to handle high-volume, real-time processing of healthcare claims data with the following key objectives:

- **Real-time Data Ingestion**: Stream medical claims data from multiple sources using Apache Kafka
- **Data Processing**: Transform and enrich claims data using Apache Spark for both streaming and batch processing
- **Data Quality**: Implement comprehensive data validation, cleansing, and quality checks
- **Analytics**: Provide real-time insights and batch analytics for claims processing efficiency
- **Scalability**: Design infrastructure that can handle millions of claims per day
- **Compliance**: Ensure HIPAA compliance and data security throughout the pipeline
- **Monitoring**: Comprehensive observability and alerting for production operations

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚â”€â”€â”€â–¶â”‚   Apache Kafka  â”‚â”€â”€â”€â–¶â”‚  Apache Spark   â”‚
â”‚                 â”‚    â”‚   (Ingestion)   â”‚    â”‚  (Processing)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚
                                â–¼                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Data Lake     â”‚    â”‚   Data Warehouseâ”‚
                       â”‚   (Raw Data)    â”‚    â”‚  (Processed)    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚
                                â–¼                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Monitoring    â”‚    â”‚   Analytics     â”‚
                       â”‚   & Alerting    â”‚    â”‚   Dashboard     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
medical-claims-data-pipeline/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ docker-compose.yml                  # Local development environment
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ .env.example                       # Environment variables template
â”œâ”€â”€ .gitignore                         # Git ignore file
â”‚
â”œâ”€â”€ infrastructure/                     # Terraform infrastructure code
â”‚   â”œâ”€â”€ main.tf                        # Main Terraform configuration
â”‚   â”œâ”€â”€ variables.tf                   # Variable definitions
â”‚   â”œâ”€â”€ outputs.tf                     # Output definitions
â”‚   â”œâ”€â”€ providers.tf                   # Provider configurations
â”‚   â”œâ”€â”€ modules/                       # Reusable Terraform modules
â”‚   â”‚   â”œâ”€â”€ kafka/                     # Kafka cluster module
â”‚   â”‚   â”œâ”€â”€ spark/                     # Spark cluster module
â”‚   â”‚   â”œâ”€â”€ monitoring/                # Monitoring stack module
â”‚   â”‚   â””â”€â”€ networking/                # Network configuration module
â”‚   â””â”€â”€ environments/                  # Environment-specific configs
â”‚       â”œâ”€â”€ dev/                       # Development environment
â”‚       â”œâ”€â”€ staging/                   # Staging environment
â”‚       â””â”€â”€ prod/                      # Production environment
â”‚
â”œâ”€â”€ src/                               # Source code
â”‚   â”œâ”€â”€ producers/                     # Data producers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ claims_producer.py         # Medical claims producer
â”‚   â”‚   â”œâ”€â”€ mock_data_generator.py     # Mock data for testing
â”‚   â”‚   â””â”€â”€ data_validator.py          # Input data validation
â”‚   â”‚
â”‚   â”œâ”€â”€ consumers/                     # Data consumers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ claims_consumer.py         # Claims data consumer
â”‚   â”‚   â””â”€â”€ batch_consumer.py          # Batch processing consumer
â”‚   â”‚
â”‚   â”œâ”€â”€ processors/                    # Data processing logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ streaming_processor.py     # Real-time processing
â”‚   â”‚   â”œâ”€â”€ batch_processor.py         # Batch processing
â”‚   â”‚   â”œâ”€â”€ data_transformer.py        # Data transformation logic
â”‚   â”‚   â””â”€â”€ quality_checker.py         # Data quality validation
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                        # Data models and schemas
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ claims_schema.py           # Claims data schema
â”‚   â”‚   â”œâ”€â”€ processed_schema.py        # Processed data schema
â”‚   â”‚   â””â”€â”€ quality_metrics.py         # Quality metrics schema
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                         # Utility functions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py                  # Configuration management
â”‚   â”‚   â”œâ”€â”€ logger.py                  # Logging configuration
â”‚   â”‚   â”œâ”€â”€ metrics.py                 # Metrics collection
â”‚   â”‚   â””â”€â”€ helpers.py                 # Helper functions
â”‚   â”‚
â”‚   â””â”€â”€ tests/                         # Test files
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ test_producers.py          # Producer tests
â”‚       â”œâ”€â”€ test_consumers.py          # Consumer tests
â”‚       â”œâ”€â”€ test_processors.py         # Processor tests
â”‚       â””â”€â”€ test_utils.py              # Utility tests
â”‚
â”œâ”€â”€ configs/                           # Configuration files
â”‚   â”œâ”€â”€ kafka/                         # Kafka configurations
â”‚   â”‚   â”œâ”€â”€ producer.properties        # Producer configuration
â”‚   â”‚   â”œâ”€â”€ consumer.properties        # Consumer configuration
â”‚   â”‚   â””â”€â”€ topics.json                # Topic definitions
â”‚   â”‚
â”‚   â”œâ”€â”€ spark/                         # Spark configurations
â”‚   â”‚   â”œâ”€â”€ spark-defaults.conf        # Spark default settings
â”‚   â”‚   â”œâ”€â”€ spark-env.sh               # Spark environment variables
â”‚   â”‚   â””â”€â”€ log4j.properties           # Logging configuration
â”‚   â”‚
â”‚   â””â”€â”€ monitoring/                    # Monitoring configurations
â”‚       â”œâ”€â”€ prometheus.yml             # Prometheus configuration
â”‚       â”œâ”€â”€ grafana/                   # Grafana dashboards
â”‚       â””â”€â”€ alertmanager.yml           # Alerting rules
â”‚
â”œâ”€â”€ scripts/                           # Utility scripts
â”‚   â”œâ”€â”€ setup.sh                       # Environment setup script
â”‚   â”œâ”€â”€ deploy.sh                      # Deployment script
â”‚   â”œâ”€â”€ health_check.sh                # Health check script
â”‚   â””â”€â”€ cleanup.sh                     # Cleanup script
â”‚
â”œâ”€â”€ docs/                              # Documentation
â”‚   â”œâ”€â”€ architecture.md                # Detailed architecture
â”‚   â”œâ”€â”€ api.md                         # API documentation
â”‚   â”œâ”€â”€ deployment.md                  # Deployment guide
â”‚   â””â”€â”€ troubleshooting.md             # Troubleshooting guide
â”‚
â””â”€â”€ data/                              # Sample data and schemas
    â”œâ”€â”€ sample_claims.json             # Sample claims data
    â”œâ”€â”€ expected_output.json           # Expected output format
    â””â”€â”€ data_dictionary.md             # Data field definitions
```

## ğŸš€ Quick Start

### Prerequisites

- Docker and Docker Compose
- Python 3.8+
- Terraform 1.0+
- AWS CLI (for cloud deployment)
- kubectl (for Kubernetes deployment)

### Local Development Setup

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd medical-claims-data-pipeline
   ```

2. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your configuration
   ```

3. **Start local infrastructure**
   ```bash
   docker-compose up -d
   ```

4. **Install Python dependencies**
   ```bash
   pip install -r requirements.txt
   ```

5. **Run the pipeline**
   ```bash
   # Start data producer
   python src/producers/mock_data_generator.py
   
   # Start streaming processor
   python src/processors/streaming_processor.py
   
   # Start batch processor
   python src/processors/batch_processor.py
   ```

## ğŸ—ï¸ Infrastructure Deployment

### Development Environment

```bash
cd infrastructure/environments/dev
terraform init
terraform plan
terraform apply
```

### Production Environment

```bash
cd infrastructure/environments/prod
terraform init
terraform plan
terraform apply
```

## ğŸ“Š Data Flow

1. **Data Ingestion**
   - Medical claims data is ingested from various sources (EMR systems, claims processors, etc.)
   - Data is validated and enriched before streaming to Kafka
   - Multiple Kafka topics handle different types of claims data

2. **Real-time Processing**
   - Apache Spark Streaming processes claims data in real-time
   - Immediate fraud detection and validation
   - Real-time metrics and alerting

3. **Batch Processing**
   - Daily batch jobs for comprehensive analytics
   - Data quality assessments and reporting
   - Historical trend analysis

4. **Data Storage**
   - Raw data stored in data lake (S3/MinIO)
   - Processed data stored in data warehouse (Redshift/Snowflake)
   - Metadata and quality metrics stored separately

5. **Analytics & Reporting**
   - Real-time dashboards for operational insights
   - Batch reports for business intelligence
   - API endpoints for data access

## ğŸ”§ Configuration

### Kafka Topics

- `medical-claims-raw`: Raw incoming claims data
- `medical-claims-validated`: Validated claims data
- `medical-claims-processed`: Processed claims data
- `medical-claims-quality`: Data quality metrics
- `medical-claims-alerts`: Fraud and anomaly alerts

### Spark Configuration

- **Streaming**: 5-second micro-batches
- **Batch**: Daily processing windows
- **Memory**: Configurable based on data volume
- **Partitioning**: Optimized for claims data patterns

## ğŸ“ˆ Monitoring & Observability

- **Metrics**: Prometheus for system and business metrics
- **Logging**: Centralized logging with ELK stack
- **Tracing**: Distributed tracing for request flows
- **Alerting**: Proactive alerting for issues
- **Dashboards**: Grafana dashboards for real-time monitoring

## ğŸ§ª Testing

```bash
# Run all tests
python -m pytest src/tests/

# Run specific test categories
python -m pytest src/tests/test_producers.py
python -m pytest src/tests/test_processors.py

# Run with coverage
python -m pytest --cov=src src/tests/
```

## ğŸ“š API Documentation

The project provides REST APIs for:
- Claims data ingestion
- Real-time analytics queries
- Batch processing status
- Data quality metrics
- System health checks

See `docs/api.md` for detailed API documentation.

## ğŸ”’ Security & Compliance

- **HIPAA Compliance**: All data handling follows HIPAA guidelines
- **Data Encryption**: Data encrypted in transit and at rest
- **Access Control**: Role-based access control (RBAC)
- **Audit Logging**: Comprehensive audit trails
- **Data Masking**: PII data masked in non-production environments

## ğŸš¨ Troubleshooting

Common issues and solutions:
- **Kafka connectivity**: Check network configuration and firewall rules
- **Spark job failures**: Review logs and resource allocation
- **Data quality issues**: Validate input data and transformation logic
- **Performance issues**: Monitor resource usage and optimize configurations

See `docs/troubleshooting.md` for detailed troubleshooting guide.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“ Support

For support and questions:
- Create an issue in the repository
- Contact the data engineering team
- Check the troubleshooting documentation

## ğŸ”„ Version History

- **v1.0.0**: Initial release with basic streaming and batch processing
- **v1.1.0**: Added data quality monitoring and alerting
- **v1.2.0**: Enhanced security and compliance features
- **v1.3.0**: Performance optimizations and monitoring improvements

---

**Note**: This is a production-ready data engineering project. Ensure proper testing and validation before deploying to production environments.
